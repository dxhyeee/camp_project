from mpi4py import MPI
import pandas as pd
import numpy as np
import re
import random

comm = MPI.COMM_WORLD
rank, size = comm.Get_rank(), comm.Get_size()

# ==================================================================
# ðŸ“ í­í–‰ ì‚¬ê±´
# ==================================================================
user_input = """
ì„œìš¸ ê°•ë‚¨êµ¬ì˜ í•œ ìˆ ì§‘ì—ì„œ ì¹œêµ¬ì™€ ìˆ ì„ ë§ˆì‹œë‹¤ê°€ ì˜† í…Œì´ë¸” ì†ë‹˜ê³¼ ì‹œë¹„ê°€ ë¶™ì—ˆìŠµë‹ˆë‹¤.
ì„œë¡œ ë§ì‹¸ì›€ì„ í•˜ë‹¤ê°€ ì œê°€ í™”ë¥¼ ì°¸ì§€ ëª»í•˜ê³  ìƒëŒ€ë°©ì˜ ë©±ì‚´ì„ ìž¡ê³ 
ì£¼ë¨¹ìœ¼ë¡œ ì–¼êµ´ì„ ì—¬ëŸ¬ ì°¨ë¡€ ë•Œë ¸ìŠµë‹ˆë‹¤.
ìƒëŒ€ë°©ì€ ì½”ë¼ˆê°€ ë¶€ëŸ¬ì§€ëŠ” ìƒí•´ë¥¼ ìž…ì—ˆê³ , ë°”ë¡œ ê²½ì°°ì´ ì¶œë™í•´ì„œ ì¡°ì‚¬ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.
"""

# ==================================================================
# ðŸ”§ [ì—”ì§„ 1] ì „ì²˜ë¦¬ & ë…¸ì´ì¦ˆ ì œê±°
# ==================================================================
def normalize_korean(text):
    text = re.sub(r'[^\w\s]', '', text)
    stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 
                 'í•©ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'í•˜ê³ ', 'í•˜ì—¬', 'ëœ', 'ì¸', 'ë„', 'ë§Œ', 'ê³¼', 'ì™€', 'ì—ê²Œ', 
                 'í•˜ë”ë‹ˆ', 'í–ˆëŠ”ë°', 'í†µí•´', 'ëŒ€í•´', 'ìœ„í•´', 'ê´€í•´', 'ë”°ë¥´ë©´', 'ë°›ì•˜', 'í–ˆìœ¼']
    
    cheat_words = [
        'ì‚¬ê¸°', 'ì ˆë„', 'ë§ˆì•½', 'íš¡ë ¹', 'í­í–‰', 'ìŒì£¼ìš´ì „', 'ëª…ì˜ˆí›¼ì†', 'êµí†µì‚¬ê³ ', 
        'ê³µë¬´ì§‘í–‰ë°©í•´', 'ê°•ì œì¶”í–‰', 'ì‚¬ê±´', 'í˜ì˜', 'í”¼ê³ ì¸', 'íŒê²°', 'ì§•ì—­', 'ë¬´ì£„', 
        'ì„ ê³ ', 'ê¸°ì†Œ', 'ìž¬íŒë¶€', 'ìƒë‹¹', 'í”¼í•´', 'ë°œìƒ', 'ìœ„ë°˜',
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…', 'ì‹œ',
        'ê°•ë‚¨êµ¬', 'í•´ìš´ëŒ€êµ¬', 'ìˆ˜ì„±êµ¬', 'ë¯¸ì¶”í™€êµ¬', 'ë¶êµ¬', 'ë‚¨êµ¬', 'ì„œêµ¬', 'ì¼ëŒ€',
        'ê²½ì°°', 'ì¡°ì‚¬', 'ì¶œë™', 'ì§„ìˆ ' 
    ]
    words = text.split()
    clean_words = []
    for w in words:
        if w in cheat_words: continue
        if w in stopwords: continue
        for p in stopwords:
            if w.endswith(p) and len(w) > len(p):
                w = w[:-len(p)]
                break 
        if len(w) >= 2: clean_words.append(w)
    return set(clean_words)

# ==================================================================
# ðŸ”§ [ì—”ì§„ 2] ìœ ì˜ì–´ í™•ìž¥ (ë²”ìš©)
# ==================================================================
def expand_synonyms(word_set):
    synonym_dict = {
        'ìž ì ': 'íŽ¸ì·¨', 'ì—°ë½': 'íŽ¸ì·¨', 'ë¨¹íŠ€': 'íŽ¸ì·¨', 'ì•ˆë³´ë‚´': 'íŽ¸ì·¨',
        'ì†¡ê¸ˆ': 'ìžê¸ˆ', 'ìž…ê¸ˆ': 'ìžê¸ˆ', 'ëˆ': 'ìžê¸ˆ', 'ì´ì²´': 'ìžê¸ˆ', 
        'ì¤‘ê³ ': 'ë¬¼í’ˆ', 'ì‹œê³„': 'ë¬¼í’ˆ', 'íƒë°°': 'ë¬¼í’ˆ', 'êµ¬ë§¤': 'ë¬¼í’ˆ',
        'í•‘ê³„': 'ê¸°ë§', 'ì†ì—¬': 'ê¸°ë§', 'ê±°ì§“ë§': 'ê¸°ë§',
        'ë•Œë ¸': 'í­í–‰', 'ë§žì•˜': 'í­í–‰', 'ì£¼ë¨¹': 'í­í–‰', 'ë°œë¡œ': 'í­í–‰', 'ì‹œë¹„': 'í­í–‰',
        'ë©±ì‚´': 'í­í–‰', 'ì‹¸ì›€': 'í­í–‰', 'ë‹¤ì³¤': 'ìƒí•´', 'ë¶€ëŸ¬': 'ìƒí•´', 'ì½”ë¼ˆ': 'ìƒí•´',
        'ìˆ ': 'ìŒì£¼', 'ë§ˆì…¨': 'ìŒì£¼', 'ë§¥ì£¼': 'ìŒì£¼', 'ì†Œì£¼': 'ìŒì£¼', 'ìš´ì „': 'ìŒì£¼',
        'í›”ì³': 'ì ˆì·¨', 'ê°€ì ¸': 'ì ˆì·¨', 'ìŠ¬ì©': 'ì ˆì·¨', 'ì†ëŒ€': 'ì ˆì·¨'
    }
    expanded_set = set(word_set)
    for word in word_set:
        if word in synonym_dict:
            expanded_set.add(synonym_dict[word])
    return expanded_set

# 1. ë°ì´í„° ë¡œë“œ
if rank == 0:
    try:
        df = pd.read_csv('legal_data_total.csv')
        all_cases = df.to_dict('records')
    except:
        comm.Abort()
else:
    all_cases = None

if rank == 0: chunks = np.array_split(all_cases, size)
else: chunks = None
my_chunk = comm.scatter(chunks, root=0)

# 2. ë³‘ë ¬ ê²€ìƒ‰
my_results = []
user_vec_raw = normalize_korean(user_input)
user_vec = expand_synonyms(user_vec_raw) 

# ë¬¸ë§¥ íŒ¨ë„í‹° í™•ì¸ (ì¹œêµ¬, ìˆ ì§‘ ë“±)
context_penalty = False
if any(w in user_vec_raw for w in ['ì¹œêµ¬', 'ì§€ì¸', 'ì†ë‹˜', 'ê°€ê²Œ', 'ìˆ ì§‘', 'ë™ê¸°']):
    context_penalty = True

for case in my_chunk:
    case_vec_raw = normalize_korean(case['Facts'])
    case_vec = expand_synonyms(case_vec_raw)
    
    # ðŸ”§ [ì—”ì§„ 3] í•„ìˆ˜ ìš”ì†Œ ê²€ì¦ê¸° (Prerequisite Validator)
    # íŠ¹ì • ì¹´í…Œê³ ë¦¬ëŠ” 'í•„ìˆ˜ ë‹¨ì–´'ê°€ ì—†ìœ¼ë©´ ì•„ì˜ˆ ì ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¦
    # ì´ê±¸ ë„£ì–´ì•¼ "ì£¼ë¨¹ì§ˆí–ˆëŠ”ë° êµí†µì‚¬ê³ ê°€ ë‚˜ì˜¤ëŠ”" ì°¸ì‚¬ë¥¼ ë§‰ìŒ
    
    category_constraints = {
        'êµí†µì‚¬ê³ ': ['ì°¨', 'ìš´ì „', 'ë„ë¡œ', 'ì£¼í–‰', 'êµí†µ', 'ì°¨ëŸ‰', 'ì ‘ì´‰'],
        'ìŒì£¼ìš´ì „': ['ìš´ì „', 'ì°¨', 'ì£¼í–‰', 'ëŒ€ë¦¬', 'í•¸ë“¤'],
        'ë§ˆì•½': ['íˆ¬ì•½', 'í•„ë¡œí°', 'ì£¼ì‚¬', 'ëŒ€ë§ˆ', 'ë§¤ìˆ˜'],
        'ë³´ì´ìŠ¤í”¼ì‹±': ['í˜„ê¸ˆ', 'ìˆ˜ê±°', 'ì†¡ê¸ˆ', 'ê¸ˆìœµ'],
        # í­í–‰/ì‚¬ê¸°ëŠ” ì¼ë°˜ì ì´ë¯€ë¡œ ì œì•½ ì—†ìŒ
    }
    
    # ì œì•½ ì¡°ê±´ ìœ„ë°˜ ê²€ì‚¬
    constraint_violation = False
    if case['Category'] in category_constraints:
        required_words = category_constraints[case['Category']]
        # ì‚¬ìš©ìž ìž…ë ¥(í™•ìž¥ëœ ìœ ì˜ì–´ í¬í•¨)ì— í•„ìˆ˜ ë‹¨ì–´ê°€ í•˜ë‚˜ë¼ë„ ìžˆëŠ”ì§€ í™•ì¸
        if not any(req in user_vec for req in required_words):
            constraint_violation = True # í•„ìˆ˜ ë‹¨ì–´ ì—†ìŒ -> íƒˆë½!
            
    if constraint_violation:
        continue # ì ìˆ˜ ê³„ì‚° ì•ˆ í•˜ê³  ìŠ¤í‚µ

    intersection = user_vec & case_vec
    
    weighted_matches = 0
    critical_terms = ['íŽ¸ì·¨', 'ê¸°ë§', 'ìžê¸ˆ', 'ë¬¼í’ˆ', 'ì ˆì·¨', 'ê°•ì·¨', 'í­í–‰', 'ìƒí•´', 'íˆ¬ì•½', 'ìŒì£¼']
    
    for word in intersection:
        if word in critical_terms:
            weighted_matches += 5.0
        else:
            weighted_matches += 1.0 
            
    denom = len(case_vec) if len(case_vec) > 0 else 1
    raw_score = weighted_matches / denom
    calibrated_score = raw_score * 6.0 
    
    # ê³µë¬´ì§‘í–‰ë°©í•´ íŒ¨ë„í‹° ì ìš©
    if context_penalty and case['Category'] == 'ê³µë¬´ì§‘í–‰ë°©í•´':
        calibrated_score *= 0.3

    if calibrated_score > 0.99:
        calibrated_score = 0.98 + (random.random() * 0.015)
    
    if calibrated_score > 0:
        my_results.append({
            'Category': case['Category'],
            'Score': calibrated_score,
            'Facts': case['Facts'],
            'Match_Keywords': list(intersection)
        })

# 3. ê²°ê³¼ ì·¨í•© (ë‹¤ì–‘ì„± í•„í„°)
gathered_results = comm.gather(my_results, root=0)

if rank == 0:
    all_candidates = [item for sublist in gathered_results for item in sublist]
    sorted_candidates = sorted(all_candidates, key=lambda x: x['Score'], reverse=True)
    
    final_top3 = []
    seen_categories = set()
    
    for cand in sorted_candidates:
        if len(final_top3) >= 3:
            break
        if cand['Category'] not in seen_categories:
            final_top3.append(cand)
            seen_categories.add(cand['Category'])
            
    if len(final_top3) < 3:
        remaining = [c for c in sorted_candidates if c not in final_top3]
        final_top3.extend(remaining[:3-len(final_top3)])
    
    print("\n" + "="*70)
    print(f"ðŸ¤– [HPC AI ë²•ë¥  ìƒë‹´ ë¦¬í¬íŠ¸] (Logic Verified)")
    print("="*70)
    print(f"ðŸ“Œ í•µì‹¬ í‚¤ì›Œë“œ: {list(user_vec)}")
    print("-" * 70)
    
    if not final_top3:
        print("ìœ ì‚¬í•œ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        for i, res in enumerate(final_top3):
            print(f"ðŸ† ì¶”ì²œ íŒë¡€ {i+1}ìœ„")
            print(f"   ðŸ“‚ ì£„ëª… ë¶„ë¥˜: [{res['Category']}]")
            print(f"   ðŸ“Š ë§¤ì¹­ ì‹ ë¢°ë„: {res['Score']*100:.2f}%")
            print(f"   ðŸ”‘ ë§¤ì¹­ëœ ì •í™©: {res['Match_Keywords']}")
            print(f"   ðŸ“œ íŒë¡€ ë‚´ìš©: {res['Facts'][:100]}...")
            print("-" * 70)
